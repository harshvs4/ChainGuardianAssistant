{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_path = \"/Users/harshsharma/Desktop/Hackathons/ChainGuardian/src/news/compliance_articles.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read json file\n",
    "import json\n",
    "with open(news_path) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[0][\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEW DELHI (Reuters) -Adani Group founder Gautam Adani responded for the first time on Saturday to allegations by U.S. authorities that he was part of a $265 ...'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API Key\n",
    "client = OpenAI(api_key=\"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\")\n",
    "# Initialize Pinecone\n",
    "pinecone = Pinecone(api_key=\"pcsk_5bwfSU_HE5WPrgGpz8D9WcFthHCpj4kggifvwtvFHhXd1jXFCBsFnqoqXYybTzn3ctxPSm\")  # Replace with your Pinecone API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or connect to a Pinecone index\n",
    "index_name = \"multi-dataset-index\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(name=index_name,\n",
    "    dimension=1536, # Replace with your model dimensions\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ))  \n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text, max_length=1000):\n",
    "    \"\"\"Truncate text to ensure it fits within metadata size limits.\"\"\"\n",
    "    return text[:max_length] if len(text) > max_length else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_metadata_size(metadata):\n",
    "    \"\"\"Calculate the size of metadata in bytes.\"\"\"\n",
    "    return sys.getsizeof(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings from both files successfully uploaded to Pinecone!\n"
     ]
    }
   ],
   "source": [
    "# Function to generate embeddings using OpenAI\n",
    "def generate_embedding(text):\n",
    "    max_token_limit = 8192  # Adjust as needed to leave room for metadata and other processing\n",
    "    truncated_text = text[:max_token_limit]\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=truncated_text,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Function to process news articles\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Helper function to sanitize text\n",
    "def sanitize_text(text):\n",
    "    \"\"\"\n",
    "    Converts text to ASCII by removing special characters and normalizing.\n",
    "    \"\"\"\n",
    "    # Normalize Unicode to ASCII and remove non-ASCII characters\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    # Remove any remaining non-alphanumeric characters, except spaces and dashes\n",
    "    text = re.sub(r\"[^\\w\\s-]\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# Function to process news articles\n",
    "def process_news_articles(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        news_articles = json.load(file)\n",
    "\n",
    "    embeddings = []\n",
    "    for article in news_articles:\n",
    "        # Sanitize title for ASCII compatibility\n",
    "        title = sanitize_text(article[\"title\"])\n",
    "        \n",
    "        # Truncate fields to reduce metadata size\n",
    "        description = truncate_text(article[\"description\"], max_length=1000)\n",
    "        content = truncate_text(article[\"content\"], max_length=10000)\n",
    "        full_text = truncate_text(article[\"full_text\"], max_length=20000)\n",
    "\n",
    "        # Generate embedding\n",
    "        text = f\"{title} {description} {content} {full_text}\"\n",
    "        embedding = generate_embedding(text)\n",
    "\n",
    "        # Metadata\n",
    "        metadata = {\n",
    "            \"url\": article[\"url\"],\n",
    "            \"dataset\": \"news\",\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"content\": content,\n",
    "            \"full_text\": full_text\n",
    "        }\n",
    "\n",
    "        # Ensure metadata size is within limits\n",
    "        if get_metadata_size(metadata) > 40960:\n",
    "            raise ValueError(f\"Metadata size exceeds limit for article: {article['url']}\")\n",
    "\n",
    "        # Append sanitized title as vector ID\n",
    "        embeddings.append((title, embedding, metadata))\n",
    "    return embeddings\n",
    "\n",
    "# Function to process transaction data\n",
    "def process_transactions(file_path):\n",
    "    transactions = pd.read_csv(file_path)\n",
    "    embeddings = []\n",
    "    for _, row in transactions.iterrows():\n",
    "        text = f\"Purpose: {row['Purpose']}, Region: {row['Region']}, Risk: {row['Risk_Category']}\"\n",
    "        embedding = generate_embedding(text)  # Generate 1536-dimensional embedding\n",
    "        metadata = {\n",
    "            \"TransactionID\": row[\"TransactionID\"],\n",
    "            \"dataset\": \"transactions\",\n",
    "            \"Purpose\": row[\"Purpose\"],\n",
    "            \"Region\": row[\"Region\"],\n",
    "            \"Risk_Category\": row[\"Risk_Category\"],\n",
    "            \"Sender\": row[\"Sender\"],\n",
    "            \"Receiver\": row[\"Receiver\"],\n",
    "            \"Amount\": row[\"Amount\"],\n",
    "            \"Currency\": row[\"Currency\"],\n",
    "            \"GasFee\": row[\"GasFee\"],\n",
    "            \"Timestamp\": row[\"Timestamp\"],\n",
    "            \"AML_KYC_Verified\": row[\"AML_KYC_Verified\"],\n",
    "            \"Geolocation_Receiver\": row[\"Geolocation_Receiver\"],\n",
    "            \"Geolocation_Sender\": row[\"Geolocation_Sender\"]\n",
    "        }\n",
    "        embeddings.append((row[\"TransactionID\"], embedding, metadata))\n",
    "    return embeddings\n",
    "\n",
    "# File paths\n",
    "news_file = \"/Users/harshsharma/Desktop/Hackathons/ChainGuardian/src/news/compliance_articles_full.json\"  # Path to the news articles JSON file\n",
    "transactions_file = \"/Users/harshsharma/Desktop/Hackathons/ChainGuardian/files/transactions.csv\"  # Path to the transactions CSV file\n",
    "\n",
    "# Process and upsert news articles\n",
    "news_embeddings = process_news_articles(news_file)\n",
    "index.upsert(vectors=news_embeddings)\n",
    "\n",
    "# Process and upsert transactions\n",
    "transaction_embeddings = process_transactions(transactions_file)\n",
    "index.upsert(vectors=transaction_embeddings)\n",
    "\n",
    "print(\"Embeddings from both files successfully uploaded to Pinecone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
